{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/Users/chukuemekaogudu/Documents/Dev-Spark-Apache/Apache-Spark/spark-2.4.5-bin-hadoop2.7\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import string\n",
    "from collections import Counter\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import re\n",
    "import math\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Volumes/oli2/inf533_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStopWords(file_path):\n",
    "    data = None\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        data = file.read()\n",
    "    return data.decode(\"utf-8\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(text, stopwords):\n",
    "    words = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = words.split()\n",
    "    \n",
    "    pattern = \"[a-zA-Z]+\"\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if re.match(pattern, word) and word not in stopwords and len(word) > 3:\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(doc, business_id):\n",
    "    word_dict = Counter(doc)\n",
    "    n_count = len(doc)\n",
    "    threshold = 3\n",
    "    \n",
    "    tf = []\n",
    "    for word, count in word_dict.items():\n",
    "        if count > threshold:\n",
    "            tf.append(((business_id, word), count / float(n_count)))\n",
    "            \n",
    "    if len(tf) == 0:\n",
    "        for word, count in word_dict.items():\n",
    "            tf.append(((business_id, word), count / float(n_count)))\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(tfidf):\n",
    "    tfidf = list(tfidf)\n",
    "    tfidf.sort(key=lambda x: x[1])\n",
    "    return [pair[0] for pair in tfidf[:200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_model(profile, model_type):\n",
    "    return [{\"description\": model_type, \"id\": k, \"profile\": v} for k, v in profile.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start = time.time()\n",
    "    config = SparkConf().setMaster(\"local[*]\") \\\n",
    "                        .setAppName(\"Task2\") \\\n",
    "                        .set(\"spark.executor.memory\", \"4g\") \\\n",
    "                        .set(\"spark.driver.memory\", \"4g\")\n",
    "    \n",
    "    sc = SparkContext(conf=config).getOrCreate()\n",
    "    \n",
    "    stopwords = loadStopWords(os.path.join(data_dir, \"stopwords\"))\n",
    "    \n",
    "    lines = sc.textFile(os.path.join(data_dir, \"train_review.json\")) \\\n",
    "              .map(json.loads).cache()\n",
    "    \n",
    "    business_text_tf = lines.map(lambda x: (x[\"business_id\"], filter_words(x[\"text\"], stopwords))) \\\n",
    "                            .reduceByKey(lambda x, y: x + y, 7) \\\n",
    "                            .flatMap(lambda x: compute_tf(x[1], x[0])) \\\n",
    "                            .cache()\n",
    "    num_doc = lines.map(lambda x: x[\"business_id\"]).distinct().count()\n",
    "    \n",
    "    business_text_idf = business_text_tf.map(lambda x: (x[0][1], x[0][0])) \\\n",
    "                                        .groupByKey() \\\n",
    "                                        .mapValues(lambda x: math.log(num_doc/ len(set(x)))) \\\n",
    "                                        .collectAsMap()\n",
    "    \n",
    "    business_tfidf = business_text_tf.map(lambda x: (x[0][0], (x[0][1], x[1] * business_text_idf[x[0][1]]))) \\\n",
    "                                     .groupByKey() \\\n",
    "                                     .mapValues(get_top_words) \\\n",
    "                                     .cache()\n",
    "    word_tokens = business_tfidf.flatMap(lambda x: x[1]).distinct().zipWithIndex().collectAsMap()\n",
    "    \n",
    "    business_profile = business_tfidf.mapValues(lambda x: [word_tokens[word] for word in x]).collectAsMap()\n",
    "    \n",
    "    user_profile = lines.map(lambda x: (x[\"user_id\"], business_profile.get(x[\"business_id\"]))) \\\n",
    "                        .filter(lambda x: x[1] != None and len(x[1]) > 0) \\\n",
    "                        .reduceByKey(lambda x, y: list(set(x)) + list(set(y))) \\\n",
    "                        .collectAsMap()\n",
    "    \n",
    "    model = convert_to_model(business_profile, \"business_profile\")\n",
    "    model += convert_to_model(user_profile, \"user_profile\")\n",
    "    \n",
    "    with open(\"output.json\", \"w+\") as file:\n",
    "        for line in model:\n",
    "            file.writelines(json.dumps(line) + \"\\n\")\n",
    "        file.close()\n",
    "    print(\"Duration \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration  139.01959323883057\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
